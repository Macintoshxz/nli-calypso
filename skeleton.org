* train.py
** NLI_Train
*** initialize_model()
**** Read in hyperparameters
*** initialize_vocab()
**** Read in vocab
*** main()
**** Load dataset (as indices)
**** Create Premise and Hypothesis objects using dimensions 
**** Create NLI_System, which hooks up Premise and Hypothesis
**** Use dataset (as indices) to train system 
***** Pass in embeddings
**** Use validation set (as indices) to train hyperparameters
***** Pass in embeddings
**** Test and check accuracy

* nli_model.py
** Premise
*** process()
**** take in embeddings as input
**** return tensor of encoded premise
** Hypothesis
*** process()
**** take in embeddings as input
**** return tensor of encoded hypothesis
** NLI_System
*** __init__
**** setup_placeholders()
***** create placeholders, including embeddings placeholder
**** setup_system()
***** link premise, hypothesis, etc. into real model
**** setup_loss()
***** calculate loss and create variable for that
*** Training
**** optimize()
***** run session one step with one batch of (x,y) data
**** train()
***** create optimizer for loss
***** run optimize in a loop over test data
*** Validation
**** test()
***** calculate cost for one batch of (x,y) data. Used for tuning hyperparameters
**** validate()
***** take validation dataset and use test() to calculate total validation cost
*** Testing
**** predict()
***** predict class (using premise + hypothesis)
**** evaluate_prediction()
***** Calculate test peformance given dataset, labels, session using a given number of samples
***** call predict() for each sample
