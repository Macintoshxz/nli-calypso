TODO
- Paramter tuning for LSTMs
- Stack LSTMs?
- Attention model

DIFFERENCES FROM LITERATURE
- We use Adam, not Adadelta. Slower but better accuracy.
- BOW: We concatonate average DWRs of both sentences with the difference between the averages.

VALIDATION PARAMETERS
- Dropout
- LR
- tanh vs relu
- Regularization lambda

DONE
- Train accuracy
- Actually run on dev set
- Dropout
- Regularization
- Run epochs to convergence
- Batch sizes for validation
- Set the random seed
- Set up LSTMs
- Set up BiLSTMs
- Pad/Mask LSTMs


BEST HYPERPARAMETERS FOR:
BOW (from grid3): Learning Rate: 0.001  Dropout Keep Rate: 0.9  Regularization lambda: 0.0001   TESTING RESULT: 0.657675081433 (50,000 train) 0.75641286645 (entire train, 0.787389850501 train accuracy) 

BEST SCORES FOR:
LSTM (10k train): Train %: 0.811. Dev %: 0.617. Dev Loss: 1.1198.


GOOD RESOURCES:
LSTMS + Bucketing:
http://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html
