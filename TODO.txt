TODO
- Set up LSTMs
- Padding / Masking?
- Paramter tuning <==
- Checkpointing
- Attention model
- Run epochs to convergence <== (DONE)
- Batch sizes for validation <== (DONE)
- Set the random seed <== (DONE)

DIFFERENCES FROM LITERATURE
- We use Adam, not Adadelta. Slower but better accuracy.
- BOW: We concatonate average DWRs of both sentences with the difference between the averages.

VALIDATION PARAMETERS
- Dropout
- LR
- tanh vs relu
- Regularization lambda

DONE
- Train accuracy
- Actually run on dev set
- Dropout
- Regularization
