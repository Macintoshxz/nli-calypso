TODO
- Train accuracy <== (DONE)
- Actually run on dev set <== (DONE)
- Dropout <== (DONE)
- Regularization <== (DONE)
- Run epochs to convergence <== (DONE)
- Batch sizes for validation <== (DONE)
- Set the random seed <== (DONE)
- Paramter tuning <==
- Set up LSTMs
- Batching with LSTMs: Padding + Masking
- Add attention
- Checkpointing

DIFFERENCES FROM LITERATURE
- We use Adam, not Adadelta. Slower but better accuracy.
- BOW: We concatonate average DWRs of both sentences with the difference between the averages.

VALIDATION PARAMETERS
- Dropout
- LR
- tanh vs relu
- Regularization lambda
