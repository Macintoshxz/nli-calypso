TODO
- Set up LSTMs
- Padding / Masking?
- Paramter tuning <==
- Checkpointing
- Attention model
- Run epochs to convergence <== (DONE)
- Batch sizes for validation <== (DONE)
- Set the random seed <== ?

DIFFERENCES FROM LITERATURE
- We use Adam, not Adadelta. Slower but better accuracy.
- BOW: We concatonate average DWRs of both sentences with the difference between the averages.

VALIDATION PARAMETERS
- Dropout
- LR
- tanh vs relu
- Regularization lambda

DONE
- Train accuracy
- Actually run on dev set
- Dropout
- Regularization


BEST HYPERPARAMETERS FOR:
BOW (from grid3): Learning Rate: 0.001  Dropout Keep Rate: 0.9  Regularization lambda: 0.0001   TESTING RESULT: 0.657675081433 (50,000 train) 0.75641286645 (entire train, 0.787389850501 train accuracy)

python code/main.py --num_train=50000 --dev --lr=0.0001 --dropout=1.0 --reg_lambd